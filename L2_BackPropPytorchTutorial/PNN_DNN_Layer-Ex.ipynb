{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> KIT Praktikum Neuronale Netze: Deep Neural Network Exercise </center>\n",
    "\n",
    "</br>\n",
    "On this exercise, you are implementing a feedforward neural network from scratch to do the MNIST problem. This one is using <code>Numpy</code>. As an exercise for Backpropagation as the Computational Graph view you learned from the practical lecture of the workshop, you should work through the derivation of the loss function to the weights or biases with pen and paper. \n",
    "\n",
    "</br>\n",
    "\n",
    "We will try do modularize our neural network as it consists of $L$ layers. **Read the following description carefully, otherwise you would have problems implementing it later**. Each layer $l$ has a weight matrix $W^{(l)}$ with the size of $(n^{(l)}, n^{(l-1)})$ where $n^{(l)}$ is the number of neurons (outputs) of the layer $l$, a bias vector $b^{(l)}$ with the size of $(n^{(l)}, 1)$. Each layer $l$ has an activation function $f$. Each layer $l$ can be the last layer or not (whose the error term is calculated differently). First, we will implement those parts for one layer:\n",
    "    \n",
    "* Initialize a layer: Use Xavier Initialization if the activation is sigmoid or tanh, otherwise use He Initialization.\n",
    "* Do the forward pass through the layer based on its activation function:\n",
    "    *  $$ z^{(l)} = W^{(l)}\\cdot a^{(l-1)} + b^{(l)} $$\n",
    "        $$ a^{(l)} = f(z^{(l)}) $$\n",
    "    *  Where:<br/>\n",
    "        $z^{(l)}$: net output of layer $l$<br/>\n",
    "        $a^{(l)}$: activation of layer $l$, is the input of the next layer $l+1$, $a^{(0)} = X$<br/>\n",
    "* Do the backward pass through the layer. Remember our convenient notion $\\check{w} = \\frac{\\partial \\mathcal{L}}{\\partial w}$? Here in the code, $\\check{w}$ is denoted by dw.<br/>\n",
    "$$\\check{w}^{(l)} = \\check{a}^{(l)} \\frac{\\partial a^{(l)}}{\\partial z^{(l)}}\\frac{\\partial z^{(l)}}{\\partial w^{(l)}}= \\check{a}^{(l)}f^{(l)'}a^{(l-1)}$$\n",
    "* Update the weights/biases based on the gradient descent principle.\n",
    "\n",
    "Then training the network is divided into several steps:\n",
    "* Initialize the whole network (by initilizing every layer)\n",
    "* Split training data into minibatches.\n",
    "* Iteratively do following steps in $E$ epochs:\n",
    "    * Shuffle minibatches.\n",
    "    * Do the forward pass.\n",
    "    * Calculate the loss.\n",
    "    * Do the backward pass.\n",
    "    * Update weights.\n",
    "    \n",
    "With the trained model, perform inference steps on the test data.  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import `numpy` and explore some basic methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement basic activation functions (vectorized version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperbolic Tangent__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectified Linear Unit (ReLU)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax for batch__:\n",
    "</br>\n",
    "\n",
    "Note that if we do minibatch update, we must compute the softmax along some dimension that is not the batch dimension. From our convention, batch dimension is the second (axis=1), so we must compute the softmax along the first dimension (axis=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_batch(z, axis=0):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "z = np.random.randn(100,2)\n",
    "\n",
    "sigmoid_z = sigmoid(z)\n",
    "tanh_z = tanh(z)\n",
    "my_tanh_z = np.tanh(z)\n",
    "relu_z = relu(z)\n",
    "linear_z = linear(z)\n",
    "softmax_z = softmax(z)\n",
    "\n",
    "\n",
    "print(\"Sigmoid results of z[0]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(sigmoid_z[0,0], sigmoid_z[6,0], sigmoid_z[8,0], sigmoid_z[22,0], sigmoid_z[98,0]))\n",
    "print(\"Tanh results of z[3]=%f, z[9]=%f, z[18]=%f, z[34]=%f, z[99]=%f\" \\\n",
    "      %(tanh_z[3,0], tanh_z[9,0], tanh_z[18,0], tanh_z[34,0], tanh_z[99,0]))\n",
    "print(\"Relu results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(relu_z[8,0], relu_z[16,0], relu_z[38,0], relu_z[72,0], relu_z[88,0]))\n",
    "print(\"Linear results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(linear_z[2,0], linear_z[6,0], linear_z[8,0], linear_z[22,0], linear_z[98,0]))\n",
    "print(\"Softmax results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(softmax_z[2,0], softmax_z[6,0], softmax_z[8,0], softmax_z[22,0], softmax_z[98,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "Sigmoid results of z[0]=0.615723, z[6]=0.721783, z[8]=0.599997, z[22]=0.561633, z[98]=0.459803 <br/>\n",
    "Tanh results of z[3]=0.696046, z[9]=0.867072, z[18]=0.124070, z[34]=0.208769, z[99]=0.280640 <br/>\n",
    "Relu results of z[8]=0.405453, z[16]=1.047579, z[38]=0.226963, z[72]=1.176812, z[88]=2.123692 <br/>\n",
    "Linear results of z[2]=-0.720589, z[6]=0.953324, z[8]=0.405453, z[22]=0.247792, z[98]=-0.161137 <br/>\n",
    "Softmax results of z[2]=0.001570, z[6]=0.008374, z[8]=0.004842, z[22]=0.004136, z[98]=0.002748 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement the derivations of basic activation functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tanh__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectified Linear Unit (ReLU)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__: \n",
    "\n",
    "This is hard to come up with without knowing the loss function. Most of the implementations out there are assuming we have cross entropy (or negative log likelihood) loss function, but it is not general enough, isn't it? So feel free to take my implementation. For your interest: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prime(z):\n",
    "    y = softmax(z).reshape(-1,1)\n",
    "    return np.diagflat(y) - np.dot(y, y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8888)\n",
    "z = np.random.randn(100,1)\n",
    "\n",
    "sigmoid_prime_z = sigmoid_prime(z)\n",
    "tanh_prime_z = tanh_prime(z)\n",
    "relu_prime_z = relu_prime(z)\n",
    "softmax_prime_z = softmax_prime(z)\n",
    "\n",
    "# The derivative of a softmax should be a Jacobian matrix\n",
    "print(softmax_prime_z.shape)\n",
    "\n",
    "print(\"Sigmoid prime results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(sigmoid_prime_z[2,0], sigmoid_prime_z[6,0], sigmoid_prime_z[8,0], sigmoid_prime_z[22,0], sigmoid_prime_z[98,0]))\n",
    "print(\"Tanh prime of z[3]=%f, z[9]=%f, z[18]=%f, z[34]=%f, z[99]=%f\" \\\n",
    "      %(tanh_prime_z[3,0], tanh_prime_z[9,0], tanh_prime_z[18,0], tanh_prime_z[34,0], tanh_prime_z[99,0]))\n",
    "print(\"Relu prime results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(relu_prime_z[8,0], relu_prime_z[16,0], relu_prime_z[38,0], relu_prime_z[72,0], relu_prime_z[88,0]))\n",
    "print(\"Softmax prime results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(softmax_prime_z[8,0], softmax_prime_z[16,0], softmax_prime_z[38,0], softmax_prime_z[72,0], softmax_prime_z[88,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "(100, 100)<br/>\n",
    "Sigmoid prime results of z[2]=0.247928, z[6]=0.228343, z[8]=0.181437, z[22]=0.249350, z[98]=0.133364<br/>\n",
    "Tanh prime of z[3]=0.027258, z[9]=0.928484, z[18]=0.357361, z[34]=0.621576, z[99]=0.568175<br/>\n",
    "Relu prime results of z[8]=1.000000, z[16]=0.000000, z[38]=1.000000, z[72]=0.000000, z[88]=1.000000<br/>\n",
    "Softmax prime results of z[8]=-0.000087, z[16]=-0.000009, z[38]=-0.000030, z[72]=-0.000010, z[88]=-0.000031"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implement basic cost (error) functions and their derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are going to calculate the error for a (mini)batch of $m$ training examples, which we assume they are [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) (i.i.d), so the (normalized) total errors of those $m$ training examples (or $m$ data points) is $E = \\frac{1}{m}\\sum_{i=1}^mE^{(i)}$, where $E^{(i)}$ is an error value of the $i^{th}$ training example. However, in order to vectorize the error over a (mini)batch, we should come up with the direct version of error function of $E$ instead of $E^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean of Squared Error__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanSquaredError(t, y):\n",
    "    # t: target label\n",
    "    # y: predicted value\n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    \n",
    "def dMSE_dy(t, y):\n",
    "    # Your implementation here!\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Categorical) Cross Entropy Error__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyError(t, y , epsilon=1e-15):\n",
    "    # t: target label of shape(n,m)\n",
    "    # y: predicted value of shape(n,m)\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "\n",
    "def dCE_dy(t, y):    \n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Binary Cross Entropy Error__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryCrossEntropyError(t, y , epsilon=1e-15):\n",
    "    # t: target label  of shape(1,m)\n",
    "    # y: predicted value of shape (1,m)\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "\n",
    "def dBCE_dy(t, y):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A handy helper function\n",
    "def create_random_onehot(n, m):\n",
    "    index = np.eye(n)\n",
    "    return index[np.random.choice(index.shape[1], size=m)].T\n",
    "t = create_random_onehot(3, 7)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward\n",
    "\n",
    "np.random.seed(1111)\n",
    "y = np.random.randn(100,50)\n",
    "sum_y = np.sum(y, axis=0, keepdims=True)\n",
    "y = y / sum_y # Create softmax-like distribution\n",
    "t = create_random_onehot(100,50)\n",
    "\n",
    "# Test forward\n",
    "myMSE = MeanSquaredError(t,y)\n",
    "myCE = CrossEntropyError(t,y)\n",
    "\n",
    "print(\"My Mean of Squared Error: \" + str(myMSE))\n",
    "print(\"My Cross Entropy Error: \" + str(myCE))\n",
    "\n",
    "yb = np.random.rand(100,50)\n",
    "tb = np.random.randint(2, size=(100,50))\n",
    "\n",
    "ybs = np.concatenate((yb, 1-yb))\n",
    "tbs = np.concatenate((tb, 1-tb))\n",
    "\n",
    "\n",
    "myBCE = BinaryCrossEntropyError(tb,yb)\n",
    "print(\"My Binary Cross Entropy Error: \" + str(myBCE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My Mean of Squared Error: 0.42921158147 <br/>\n",
    "My Cross Entropy Error: 15.8303339039 <br/>\n",
    "My Binary Cross Entropy Error: 1.04915015195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backward\n",
    "\n",
    "dMSE_dy1 = dMSE_dy(t, y)\n",
    "dCE_dy1 =  dCE_dy(t, y)\n",
    "dBCE_dy1 =  dBCE_dy(tb, yb)\n",
    "    \n",
    "print(\"My Mean of Squared Derivation dMSE_dy at dMSE_dy[2,4]=%f, dMSE_dy[6,5]=%f, dMSE_dy[8,20]=%f, dMSE_dy[22,7]=%f, dMSE_dy[98,40]=%f\" \\\n",
    "      %(dMSE_dy1[2,4], dMSE_dy1[6,5], dMSE_dy1[8,20], dMSE_dy1[22,7], dMSE_dy1[98,40]))\n",
    "print(\"My Cross Entropy Derivation: dCE_dy at dCE_dy[2,4]=%f, dCE_dy[6,5]=%f, dCE_dy[8,20]=%f, dCE_dy[22,7]=%f, dCE_dy[98,40]=%f\" \\\n",
    "      %(dCE_dy1[2,4], dCE_dy1[6,5], dCE_dy1[8,20], dCE_dy1[22,7], dCE_dy1[98,40]))\n",
    "print(\"My Binar Cross Entropy Derivation: dBCE_dy at dBCE_dy[2]=%f, dBCE_dy[6]=%f, dBCE_dy[20]=%f, dBCE_dy[22]=%f, dBCE_dy[40]=%f\" \\\n",
    "      %(dBCE_dy1[2], dBCE_dy1[6], dBCE_dy1[20], dBCE_dy1[22], dBCE_dy1[40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My Mean of Squared Derivation dMSE_dy at dMSE_dy[2,4]=0.000000, dMSE_dy[6,5]=0.000618, dMSE_dy[8,20]=-0.000168, dMSE_dy[22,7]=0.000003, dMSE_dy[98,40]=0.000039<br/>\n",
    "My Cross Entropy Derivation: dCE_dy at dCE_dy[2,4]=0.000018, dCE_dy[6,5]=0.030883, dCE_dy[8,20]=-0.008391, dCE_dy[22,7]=0.000130, dCE_dy[98,40]=0.001969<br/>\n",
    "My Binar Cross Entropy Derivation: dBCE_dy at dBCE_dy[2]=0.100696, dBCE_dy[6]=-0.671816, dBCE_dy[20]=0.031237, dBCE_dy[22]=-0.106447, dBCE_dy[40]=-0.052379<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Layer Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Initialization\n",
    "\n",
    "Initialize a layer: Use Xavier Initialization if the activation is sigmoid or tanh, otherwise use He Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(n_in, n_out, activation_string):\n",
    "    \"\"\"\n",
    "    Implement the initialization for our layer\n",
    "\n",
    "    Arguments:\n",
    "    n_in -- size of previous layer\n",
    "    n_out -- size of current layer\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_string == 'sigmoid' or activation_string == 'tanh' or activation_string == 'softmax':\n",
    "        # Xavier Initialization\n",
    "        W = np.random.randn(n_out, n_in) * (np.sqrt(2. / (n_in + n_out)))\n",
    "        b = np.zeros((n_out, 1))\n",
    "    else: \n",
    "        # He Initialization\n",
    "        W = np.random.randn(n_out, n_in) * (np.sqrt(1. / n_in))\n",
    "        b = np.zeros((n_out, 1))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A_prev, W, b, activation_string):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for our layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data X): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- values of (A_prev, Z, W) we store for computing backward propagation efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    \n",
    "    #return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test forward__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2222)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "\n",
    "A, cache = forward(A_prev, W, b, \"sigmoid\")\n",
    "print(\"With sigmoid: Z = %s , A = %s\" % (str(cache[1]), str(A)))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"tanh\")\n",
    "print(\"With tanh: A = \" + str(A))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"softmax\")\n",
    "print(\"With softmax: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "With sigmoid: Z = [[ 1.1337088  -1.20334105]] , A = [[ 0.75652269  0.2308814 ]]<br/>\n",
    "With tanh: A = [[ 0.81228478 -0.83467086]]<br/>\n",
    "With ReLU: A = [[ 1.1337088  0.       ]]<br/>\n",
    "With softmax: A = [[ 0.91189936  0.08810064]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for our layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- activation gradient for current layer l \n",
    "    cache -- values of (A_prev, Z, W) we store for computing backward propagation efficiently\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    \n",
    "    #return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Backward__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1111)\n",
    "dA = np.random.randn(1,2)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z = np.random.randn(1,2)\n",
    "\n",
    "cache_sigmoid = (A_prev, Z, W, \"sigmoid\")\n",
    "cache_tanh = (A_prev, Z, W, \"tanh\")\n",
    "cache_relu = (A_prev, Z, W, \"relu\")\n",
    "cache_softmax = (A_prev, Z, W, \"softmax\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_sigmoid)\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_tanh)\n",
    "print (\"tanh:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_relu)\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_softmax)\n",
    "print (\"softmax:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "sigmoid:<br/>\n",
    "dA_prev = [[ 0.38601836  0.4093035 ]<br/>\n",
    " [ 0.04975095  0.05275199]<br/>\n",
    " [ 0.37804364  0.40084773]]<br/>\n",
    "dW = [[-0.01627613 -0.14937127  0.091592  ]]<br/>\n",
    "db = [[-0.25163402]]<br/>\n",
    "<br/>\n",
    "tanh:<br/>\n",
    "dA_prev = [[ 0.74460672  1.47719476]<br/>\n",
    " [ 0.09596665  0.19038431]<br/>\n",
    " [ 0.72922394  1.4466775 ]]<br/>\n",
    "dW = [[-0.25178866 -0.44375637  0.44336361]]<br/>\n",
    "db = [[-0.70296173]]<br/>\n",
    "<br/>\n",
    "relu:<br/>\n",
    "dA_prev = [[ 2.05442539  0.        ]<br/>\n",
    " [ 0.26477914  0.        ]<br/>\n",
    " [ 2.01198317  0.        ]]<br/>\n",
    "dW = [[ 0.6115193  -0.30198246 -0.35733097]]<br/>\n",
    "db = [[-0.65000516]]<br/>\n",
    "\n",
    "softmax:<br/>\n",
    "dA_prev = [[ 0.05452866 -0.05452866]<br/>\n",
    " [ 0.00702778 -0.00702778]<br/>\n",
    " [ 0.05340215 -0.05340215]]<br/>\n",
    "dW = [[-0.02878513 -0.02632298 -0.02143347]]<br/>\n",
    "db = [[ -1.38777878e-17]]<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Update parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(lr, W, b, dW, db):\n",
    "    W1 = W - lr * dW \n",
    "    b1 = b - lr * db\n",
    "    return W1, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Network Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_initialize(layer_sizes = [768,100,50,10], activations = [\"relu\",\"relu\",\"softmax\"], seed=9999):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of a network.\n",
    "    \n",
    "    Arguments:\n",
    "    layer_sizes -- A list of layer sizes. \n",
    "    activations -- A list of corresponding activation functions.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Dictionary of initialized weights and biases for every layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "        \n",
    "    #return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "parameters = network_initialize([2,3,4],[\"relu\",\"softmax\"])\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "{ &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W1': array([[-0.36418784, &nbsp;  &nbsp;  0.41853418], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [ 0.03385161, &nbsp;  &nbsp; -0.34535427], <br/>\n",
    "   &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.25098956, &nbsp;  &nbsp; -0.27705387]]), <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W2': array([[ 0.99648945, &nbsp;  &nbsp; -0.73309211, &nbsp;  &nbsp;  1.18999424], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [-0.06313226,  &nbsp;  &nbsp; 0.06406165, &nbsp;  &nbsp;  0.09760321], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.35157642, &nbsp;  &nbsp; -0.87994782, &nbsp;  &nbsp; -0.61020235], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.76922564, &nbsp;  &nbsp;  0.39821514, &nbsp;  &nbsp; -0.92965227]]), <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;    'act1': 'relu', <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;     'act2': 'softmax', <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;     'b1': array([[ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.]]), <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;   'b2': array([[ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.]])} <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Network Forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Do the forward pass through the layers.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- The input of the size (n, m): n features and m instances (m = batch size).\n",
    "    parameters -- The weights and biases of every layers in the network.\n",
    "    \n",
    "    Returns:\n",
    "    A -- Final activations (output activations).\n",
    "    caches -- the cached values for faster calculation of the backward step later.\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev = X\n",
    "    L = len(parameters) // 3 # Number of layers in the network\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(L):\n",
    "        # Your implementation here!\n",
    "        pass\n",
    "    \n",
    "    #return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(5,4)\n",
    "W1 = np.random.randn(4,5)\n",
    "b1 = np.random.randn(4,1)\n",
    "W2 = np.random.randn(3,4)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(1,3)\n",
    "b3 = np.random.randn(1,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"relu\",\n",
    "              \"act3\": \"softmax\"}\n",
    "AL, caches = network_forward(X, parameters)    \n",
    "print(\"AL = \" + str(AL))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "AL = [[ 0.01497328  0.87662976  0.09019153  0.01820543]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Calculate the error and error term of the last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(T, AL, error_string):\n",
    "    \"\"\"\n",
    "    Calculate the error and the error term of the last layer.\n",
    "    \n",
    "    Arguments:\n",
    "    T -- The target labels of the size (n, m): n features and m instances (m = batch size).\n",
    "    AL -- Final activations (output activations from the last layer).\n",
    "    error_string -- The string representing the error function: \"bce\", \"mse\" or \"ce\".\n",
    "    \n",
    "    Returns:\n",
    "    error -- The error value.\n",
    "    dAL -- the error term (the derivative of the error w.r.t the activation) of the last layer.\n",
    "    \"\"\"\n",
    "    if error_string == \"mse\":\n",
    "        error = MeanSquaredError(T, AL)\n",
    "        dAL = dMSE_dy(T, AL)\n",
    "    elif error_string == 'bce':\n",
    "        error = BinaryCrossEntropyError(T, AL)\n",
    "        dAL = dBCE_dy(T, AL)\n",
    "    elif error_string == 'ce':\n",
    "        error = CrossEntropyError(T, AL)\n",
    "        dAL = dCE_dy(T, AL)\n",
    "    else:\n",
    "        raise NameError(\"Your error string '%s' is undefined!\" % error_string)\n",
    "        \n",
    "    return error, dAL\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Network Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_backward(dAL, caches):\n",
    "    \"\"\"\n",
    "    Do the backward pass through the layers.\n",
    "    \n",
    "    Arguments:\n",
    "    dAL -- the error term (the derivative of the error w.r.t the activation) of the last layer.\n",
    "    caches -- the cached values from the forward step before.\n",
    "   \n",
    "    Returns:\n",
    "    grads -- a dictionary to store the calculated derivatives of the error w.r.t to the weights and biases.\n",
    "    \"\"\"\n",
    "    L = len(caches) # Number of layers in the network\n",
    "    grads = {}\n",
    "    \n",
    "    dA = dAL\n",
    "    \n",
    "    for l in reversed(range(1,L+1)):\n",
    "        # Your implementation here!\n",
    "        pass\n",
    "    \n",
    "    #return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(7,6)\n",
    "W1 = np.random.randn(5,7)\n",
    "b1 = np.random.randn(5,1)\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(1,3)\n",
    "b3 = np.random.randn(1,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"sigmoid\",\n",
    "              \"act3\": \"softmax\"}\n",
    "\n",
    "t = create_random_onehot(1,6)\n",
    "\n",
    "\n",
    "\n",
    "AL, caches = network_forward(X, parameters)\n",
    "\n",
    "print(\"My activation last layer AL:\\n\", AL)\n",
    "error, dAL = calculate_error(t, AL, \"ce\")\n",
    "\n",
    "\n",
    "print(\"========================\")\n",
    "print(\" My error:\" + str(error))\n",
    "\n",
    "my_A2, my_Z3, _, _ = caches[2]\n",
    "\n",
    "#my_dZ3 = dAL * sigmoid_prime(my_Z3)\n",
    "\n",
    "my_dZ3 = dAL * softmax(my_Z3)\n",
    "s = my_dZ3.sum(axis=my_dZ3.ndim - 1, keepdims=True)\n",
    "my_dZ3 -= s * softmax(my_Z3) \n",
    "\n",
    "\n",
    "\n",
    "m = 1 # my_A2.shape[1]\n",
    "my_dW3 = np.dot(my_dZ3, my_A2.T)\n",
    "my_db3 = np.sum(my_dZ3, axis=1, keepdims=True)\n",
    "my_dA2 = np.dot(W3.T, my_dZ3)\n",
    "\n",
    "\n",
    "my_A1, my_Z2, _, _ = caches[1]\n",
    "my_dZ2 = my_dA2 * relu_prime(my_Z2)\n",
    "my_dW2 = np.dot(my_dZ2, my_A1.T)\n",
    "my_db2 = np.sum(my_dZ2, axis=1, keepdims=True)\n",
    "my_dA1 = np.dot(W2.T, my_dZ2)\n",
    "\n",
    "my_A0, my_Z1, _, _ = caches[0]\n",
    "my_dZ1 = my_dA1 * relu_prime(my_Z1)\n",
    "my_dW1 = np.dot(my_dZ1, my_A0.T)\n",
    "my_db1 = np.sum(my_dZ1, axis=1, keepdims=True)\n",
    "my_dA0 = np.dot(W1.T, my_dZ1)\n",
    "\n",
    "\n",
    "print(\"========================\")\n",
    "print(\"My W1 grad dW1:\\n\" + str(torch.from_numpy(my_dW1))) \n",
    "print(\"My b1 grad db1:\\n\" + str(torch.from_numpy(my_db1)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My activation last layer AL: <br/>\n",
    " &nbsp;  &nbsp; [[ 0.21921768  0.11988903  0.16260051  0.12001303  0.24447803  0.13380173]] <br/>\n",
    "======================== <br/>\n",
    " &nbsp; My error:1.83258646479 <br/>\n",
    "======================== <br/>\n",
    "My W1 grad dW1: <br/>\n",
    "tensor([[-0.0010, -0.0042,  0.0002,  0.0012, -0.0027, -0.0008,  0.0011], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [-0.0017,  0.0006,  0.0001,  0.0011,  0.0006,  0.0009,  0.0007], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;        [-0.0009,  0.0015, -0.0012, -0.0013,  0.0005,  0.0006,  0.0004], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;          [ 0.0012, -0.0007,  0.0009,  0.0004, -0.0005, -0.0013, -0.0013], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [ 0.0002, -0.0002,  0.0003,  0.0005,  0.0001, -0.0000, -0.0003]], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp;        dtype=torch.float64) <br/>\n",
    "My b1 grad db1: <br/>\n",
    "tensor([[-0.0037], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [ 0.0013], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [-0.0020], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [ 0.0017], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [ 0.0007]], dtype=torch.float64) <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4. Update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_update(lr, parameters, grads):\n",
    "    \"\"\"\n",
    "    Update the parameters of all layers.\n",
    "    \n",
    "    Arguments:\n",
    "    lr -- learning rate.\n",
    "    parameters -- the parameters of the network to be updated.\n",
    "    grads -- a dictionary to store the calculated derivatives of the error w.r.t to the parameters.\n",
    "   \n",
    "    Returns:\n",
    "    parameters -- the updated parameters of the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 3 # Number of layers in the network\n",
    "    for l in range(L):\n",
    "        # Your implementation here!\n",
    "        pass\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(7,10)\n",
    "W1 = np.random.randn(5,7)\n",
    "b1 = np.random.randn(5,1)\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.random.randn(3,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"softmax\"}\n",
    "\n",
    "t = create_random_onehot(3,10)\n",
    "\n",
    "\n",
    "AL, caches = network_forward(X, parameters)\n",
    "print(np.sum(AL,axis=0))\n",
    "error, dAL = calculate_error(t, AL, \"ce\")\n",
    "grads = network_backward(dAL, caches)\n",
    "\n",
    "parameters = network_update(0.5, parameters, grads)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(parameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "{&nbsp;  &nbsp;  &nbsp;    'W1': array([[-1.03295651,  0.35862528,  1.07353604, -0.37522946,  0.39616932, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;       -0.47148865,  2.33632501], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 1.50286388, -0.59545956,  0.52839833,  0.93994217,  0.42634415, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;        -0.75807972, -0.16243501], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [ 0.72680932,  0.44408241, -0.85682286,  0.44692782, -1.01464803, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;      -2.13232319,  0.17386349], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.95139218,  0.4419052 ,  1.46915517,  1.7497915 ,  0.35367386, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;      -0.64316151, -0.04739572], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [-1.44902964, -0.03619117, -0.09084896,  0.17629557,  1.09461738, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;      -2.12647575,  0.75144374]]), <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W2': array([[-0.73957208,  0.52294535, -0.59187648, -0.47748711,  0.11252968], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 1.90396151,  0.69458768, -0.01951732,  1.66320563,  0.02993579], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [-0.29749921, -0.96813764,  0.16706695,  0.11660199, -0.68225729]]), <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;    'act1': 'relu', <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;   'act2': 'softmax', <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;    'b1': array([[-0.54084024], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.79330547], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.17365274], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [-1.03523086], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.87426565]]), <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;    'b2': array([[-1.91402115], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [-0.13990231], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.11492465]])} <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Read MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../Data/mnist_seven.csv\"\n",
    "data = np.genfromtxt(data_path, delimiter=\",\", dtype=\"uint8\")\n",
    "train, dev, test = data[:4000], data[4000:4500], data[4500:]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    X = dataset[:, 1:] / 255.     # Normalize input features\n",
    "    Y_temp = dataset[:, 0]\n",
    "    \n",
    "    # Convert labels to one-hot vectors\n",
    "    n_values = np.max(Y_temp) + 1\n",
    "    Y = np.eye(n_values)[Y_temp]\n",
    "\n",
    "    return X.T, Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = normalize(train)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_test, Y_test = normalize(test)\n",
    "print(X_test.shape)\n",
    "print(Y_test[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "# Train\n",
    "print(\"Training...\")\n",
    "np.random.seed(1234)\n",
    "parameters = network_initialize(layer_sizes = [784,100,50,10], activations = [\"sigmoid\", \"sigmoid\", \"softmax\"])\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    L = 0\n",
    "    \n",
    "    # Split into minibatches into a *list* of sub-arrays\n",
    "    # we want to split along the number of instances, so axis = 1\n",
    "    X_minibatch = np.array_split(X_train, batch_size, axis = 1)\n",
    "    Y_minibatch = np.array_split(Y_train, batch_size, axis = 1) \n",
    "\n",
    "    # We shuffle the minibatches of X and Y in the same way\n",
    "    nmb = len(X_minibatch) # number of minibatches\n",
    "    np.random.seed(8888)\n",
    "    shuffled_index = np.random.permutation(range(nmb))\n",
    "\n",
    "    # Now we can do the training, we cannot vectorize over different minibatches\n",
    "    # They are like our \"epochs\"\n",
    "    for i in range(nmb):\n",
    "        X_current = X_minibatch[shuffled_index[i]]\n",
    "        Y_current = Y_minibatch[shuffled_index[i]]         \n",
    "            \n",
    "    #   Those two commented lines are for training Batch GD   \n",
    "    #   AL, caches = network_forward(X_train, parameters)\n",
    "    #   error, dAL = calculate_error(Y_train, AL, \"ce\")\n",
    "        AL, caches = network_forward(X_current, parameters)\n",
    "        error, dAL = calculate_error(Y_current, AL, \"ce\")\n",
    "        grads = network_backward(dAL, caches)\n",
    "        parameters = network_update(15, parameters, grads)\n",
    "        L += error\n",
    "        \n",
    "    print(\"Error of the epoch {0}: {1}\".format(epoch + 1, L/batch_size))\n",
    "\n",
    "print(\"...Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test = X_test.shape[1]\n",
    "AL_test, _ = network_forward(X_test, parameters)\n",
    "correct = (np.argmax(Y_test, axis=0) == np.argmax(AL_test, axis=0)).sum()\n",
    "   \n",
    "print('Accuracy on the test images: %d %%' % (100 * correct / m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should produce a 93% accuracy, which is not bad. Just a little bit worse than the pytorch version (94%) whose difference is the usage of Adam (a better optimization technique) compared to our simple SGD. Notice the learning rate. Change it and play with it. In our simple SGD-based model, it is an important hyperparameter and the performance of our model is learning rate-sensitive. **Do you agree with me that PyTorch already helps us a lot on this simple feed-forward architecture?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
