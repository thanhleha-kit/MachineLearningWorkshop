{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> KIT Praktikum Neuronale Netze: Deep Neural Network Solution </center>\n",
    "\n",
    "</br>\n",
    "On this exercise, you are implementing a feedforward neural network from scratch to do the MNIST problem. This one is using <code>Numpy</code>. As an exercise for Backpropagation as the Computational Graph view you learned from the practical lecture of the workshop, you should work through the derivation of the loss function to the weights or biases with pen and paper.   \n",
    "</br>\n",
    "\n",
    "We will try do modularize our neural network as it consists of $L$ layers. **Read the following description carefully, otherwise you would have problems implementing it later**. Each layer $l$ has a weight matrix $W^{(l)}$ with the size of $(n^{(l)}, n^{(l-1)})$ where $n^{(l)}$ is the number of neurons (outputs) of the layer $l$, a bias vector $b^{(l)}$ with the size of $(n^{(l)}, 1)$. Each layer $l$ has an activation function $f$. Each layer $l$ can be the last layer or not (whose the error term is calculated differently). First, we will implement those parts for one layer:\n",
    "    \n",
    "* Initialize a layer: Use Xavier Initialization if the activation is sigmoid or tanh, otherwise use He Initialization.\n",
    "* Do the forward pass through the layer based on its activation function:\n",
    "    *  $$ z^{(l)} = W^{(l)}\\cdot a^{(l-1)} + b^{(l)} $$\n",
    "        $$ a^{(l)} = f(z^{(l)}) $$\n",
    "    *  Where:<br/>\n",
    "        $z^{(l)}$: net output of layer $l$<br/>\n",
    "        $a^{(l)}$: activation of layer $l$, is the input of the next layer $l+1$, $a^{(0)} = X$<br/>\n",
    "* Do the backward pass through the layer. Remember our convenient notion $\\check{w} = \\frac{\\partial \\mathcal{L}}{\\partial w}$? Here in the code, $\\check{w}$ is denoted by dw.<br/>\n",
    "$$\\check{w}^{(l)} = \\check{a}^{(l)} \\frac{\\partial a^{(l)}}{\\partial z^{(l)}}\\frac{\\partial z^{(l)}}{\\partial w^{(l)}}= \\check{a}^{(l)}f^{(l)'}a^{(l-1)}$$\n",
    "* Update the weights/biases based on the gradient descent principle.\n",
    "\n",
    "Then training the network is divided into several steps:\n",
    "* Initialize the whole network (by initilizing every layer)\n",
    "* Split training data into minibatches.\n",
    "* Iteratively do following steps in $E$ epochs:\n",
    "    * Shuffle minibatches.\n",
    "    * Do the forward pass.\n",
    "    * Calculate the loss.\n",
    "    * Do the backward pass.\n",
    "    * Update weights.\n",
    "    \n",
    "With the trained model, perform inference steps on the test data.  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import `numpy` and explore some basic methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement basic activation functions (vectorized version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperbolic Tangent__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectified Linear Unit (ReLU)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    ez = np.exp(z - np.max(z))\n",
    "    return ez / np.sum(ez)\n",
    "    # The original formular is less preferred due to numerical unstability:\n",
    "    #ez = np.exp(z)\n",
    "    #return ez / np.sum(ez)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax for batch__:\n",
    "</br>\n",
    "\n",
    "Note that if we do minibatch update, we must compute the softmax along some dimension that is not the batch dimension. From our convention, batch dimension is the second (axis=1), so we must compute the softmax along the first dimension (axis=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_batch(z, axis=0):\n",
    "    ez = np.exp(z - np.max(z, axis=axis, keepdims=True))\n",
    "    return ez / np.sum(ez, axis=axis, keepdims=True)\n",
    "    # The original formular is less preferred due to numerical unstability:\n",
    "    #ez = np.exp(z)\n",
    "    #return ez / np.sum(ez, axis=axis, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid results of z[0]=0.615723, z[6]=0.721783, z[8]=0.599997, z[22]=0.561633, z[98]=0.459803\n",
      "Tanh results of z[3]=0.696046, z[9]=0.867072, z[18]=0.124070, z[34]=0.208769, z[99]=0.280640\n",
      "Relu results of z[8]=0.405453, z[16]=1.047579, z[38]=0.226963, z[72]=1.176812, z[88]=2.123692\n",
      "Linear results of z[2]=-0.720589, z[6]=0.953324, z[8]=0.405453, z[22]=0.247792, z[98]=-0.161137\n",
      "Softmax results of z[2]=0.001570, z[6]=0.008374, z[8]=0.004842, z[22]=0.004136, z[98]=0.002748\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "z = np.random.randn(100,2)\n",
    "\n",
    "sigmoid_z = sigmoid(z)\n",
    "tanh_z = tanh(z)\n",
    "my_tanh_z = np.tanh(z)\n",
    "relu_z = relu(z)\n",
    "linear_z = linear(z)\n",
    "softmax_z = softmax(z)\n",
    "\n",
    "\n",
    "print(\"Sigmoid results of z[0]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(sigmoid_z[0,0], sigmoid_z[6,0], sigmoid_z[8,0], sigmoid_z[22,0], sigmoid_z[98,0]))\n",
    "print(\"Tanh results of z[3]=%f, z[9]=%f, z[18]=%f, z[34]=%f, z[99]=%f\" \\\n",
    "      %(tanh_z[3,0], tanh_z[9,0], tanh_z[18,0], tanh_z[34,0], tanh_z[99,0]))\n",
    "print(\"Relu results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(relu_z[8,0], relu_z[16,0], relu_z[38,0], relu_z[72,0], relu_z[88,0]))\n",
    "print(\"Linear results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(linear_z[2,0], linear_z[6,0], linear_z[8,0], linear_z[22,0], linear_z[98,0]))\n",
    "print(\"Softmax results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(softmax_z[2,0], softmax_z[6,0], softmax_z[8,0], softmax_z[22,0], softmax_z[98,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "Sigmoid results of z[0]=0.615723, z[6]=0.721783, z[8]=0.599997, z[22]=0.561633, z[98]=0.459803 <br/>\n",
    "Tanh results of z[3]=0.696046, z[9]=0.867072, z[18]=0.124070, z[34]=0.208769, z[99]=0.280640 <br/>\n",
    "Relu results of z[8]=0.405453, z[16]=1.047579, z[38]=0.226963, z[72]=1.176812, z[88]=2.123692 <br/>\n",
    "Linear results of z[2]=-0.720589, z[6]=0.953324, z[8]=0.405453, z[22]=0.247792, z[98]=-0.161137 <br/>\n",
    "Softmax results of z[2]=0.001570, z[6]=0.008374, z[8]=0.004842, z[22]=0.004136, z[98]=0.002748 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement the derivations of basic activation functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tanh__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(z):\n",
    "    return (1 - tanh(z)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectified Linear Unit (ReLU)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_prime(z):\n",
    "    return np.logical_not(z < 0)  # Theoretically if z = 0, relu_prime should return UNDEFINED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prime(z):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__: \n",
    "\n",
    "This is hard to come up with without knowing the loss function. Most of the implementations out there are assuming we have cross entropy (or negative log likelihood) loss function, but it is not general enough, isn't it? So feel free to take my implementation. For your interest: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prime(z):\n",
    "    y = softmax(z).reshape(-1,1)\n",
    "    return np.diagflat(y) - np.dot(y, y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax for batch__: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prime_batch(z, axis=0):\n",
    "\n",
    "    result = np.empty((z.shape[axis], z.shape[axis],0))\n",
    "    for i in range(z.shape[1-axis]):\n",
    "        y1 = softmax_prime(z[:,i])\n",
    "        result= np.concatenate((result,y1.reshape(z.shape[axis],z.shape[axis],1)), axis=(2-axis))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "Sigmoid prime results of z[2]=0.247928, z[6]=0.228343, z[8]=0.181437, z[22]=0.249350, z[98]=0.133364\n",
      "Tanh prime of z[3]=0.027258, z[9]=0.928484, z[18]=0.357361, z[34]=0.621576, z[99]=0.568175\n",
      "Relu prime results of z[8]=1.000000, z[16]=0.000000, z[38]=1.000000, z[72]=0.000000, z[88]=1.000000\n",
      "Softmax prime results of z[8]=-0.000087, z[16]=-0.000009, z[38]=-0.000030, z[72]=-0.000010, z[88]=-0.000031\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(8888)\n",
    "z = np.random.randn(100,1)\n",
    "\n",
    "sigmoid_prime_z = sigmoid_prime(z)\n",
    "tanh_prime_z = tanh_prime(z)\n",
    "relu_prime_z = relu_prime(z)\n",
    "softmax_prime_z = softmax_prime(z)\n",
    "\n",
    "# The derivative of a softmax should be a Jacobian matrix\n",
    "print(softmax_prime_z.shape)\n",
    "\n",
    "print(\"Sigmoid prime results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(sigmoid_prime_z[2,0], sigmoid_prime_z[6,0], sigmoid_prime_z[8,0], sigmoid_prime_z[22,0], sigmoid_prime_z[98,0]))\n",
    "print(\"Tanh prime of z[3]=%f, z[9]=%f, z[18]=%f, z[34]=%f, z[99]=%f\" \\\n",
    "      %(tanh_prime_z[3,0], tanh_prime_z[9,0], tanh_prime_z[18,0], tanh_prime_z[34,0], tanh_prime_z[99,0]))\n",
    "print(\"Relu prime results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(relu_prime_z[8,0], relu_prime_z[16,0], relu_prime_z[38,0], relu_prime_z[72,0], relu_prime_z[88,0]))\n",
    "print(\"Softmax prime results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(softmax_prime_z[8,0], softmax_prime_z[16,0], softmax_prime_z[38,0], softmax_prime_z[72,0], softmax_prime_z[88,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "(100, 100)<br/>\n",
    "Sigmoid prime results of z[2]=0.247928, z[6]=0.228343, z[8]=0.181437, z[22]=0.249350, z[98]=0.133364<br/>\n",
    "Tanh prime of z[3]=0.027258, z[9]=0.928484, z[18]=0.357361, z[34]=0.621576, z[99]=0.568175<br/>\n",
    "Relu prime results of z[8]=1.000000, z[16]=0.000000, z[38]=1.000000, z[72]=0.000000, z[88]=1.000000<br/>\n",
    "Softmax prime results of z[8]=-0.000087, z[16]=-0.000009, z[38]=-0.000030, z[72]=-0.000010, z[88]=-0.000031"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implement basic cost (error) functions and their derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are going to calculate the error for a (mini)batch of $m$ training examples, which we assume they are [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) (i.i.d), so the (normalized) total errors of those $m$ training examples (or $m$ data points) is $E = \\frac{1}{m}\\sum_{i=1}^mE^{(i)}$, where $E^{(i)}$ is an error value of the $i^{th}$ training example. However, in order to vectorize the error over a (mini)batch, we should come up with the direct version of error function of $E$ instead of $E^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean of Squared Error__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanSquaredError(t, y):\n",
    "    # t: target label\n",
    "    # y: predicted value\n",
    "    n, m = y.shape\n",
    "    return np.sum((y - t)**2) / (n * m)\n",
    "    # or a better solution:\n",
    "    #return np.mean(np.square(y - t))\n",
    "    \n",
    "def dMSE_dy(t, y):\n",
    "    n, m = y.shape\n",
    "    return 2 * (y-t) / (n * m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Categorical) Cross Entropy Error__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyError(t, y , epsilon=1e-15):\n",
    "    # t: target label of shape(n,m)\n",
    "    # y: predicted value of shape(n,m)\n",
    "    \n",
    "    # To avoid numerical issues\n",
    "    y = np.clip(y, epsilon, 1)\n",
    "    \n",
    "    # Remember to divide it to m\n",
    "    m = y.shape[1]   \n",
    "    return -np.sum(t * np.log(y)) / m\n",
    "\n",
    "def dCE_dy(t, y):\n",
    "\n",
    "    m = y.shape[1]\n",
    "    return (y - t)/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Binary Cross Entropy Error__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryCrossEntropyError(t, y , epsilon=1e-15):\n",
    "    # t: target label  of shape(1,m)\n",
    "    # y: predicted value of shape (1,m)\n",
    "    \n",
    "    # To avoid numerical issues\n",
    "    y = np.clip(y, epsilon, 1)\n",
    "    \n",
    "    # Remember to divide it to m\n",
    "    m = y.shape[1]   \n",
    "    return np.sum(-t[0,:] * np.log(y[0,:]) - (1 - t[0,:]) * np.log(1 - y[0,:])) / m\n",
    "\n",
    "def dBCE_dy(t, y):\n",
    "    m = y.shape[1]\n",
    "    return (y[0,:] - t[0,:]) / ((y[0,:] * m) * (1 - y[0,:])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# A handy helper function\n",
    "def create_random_onehot(n, m):\n",
    "    index = np.eye(n)\n",
    "    return index[np.random.choice(index.shape[1], size=m)].T\n",
    "t = create_random_onehot(3, 7)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Mean of Squared Error: 0.42921158147\n",
      "My Cross Entropy Error: 15.8303339039\n",
      "My Binary Cross Entropy Error: 1.04915015195\n"
     ]
    }
   ],
   "source": [
    "# Test forward\n",
    "\n",
    "np.random.seed(1111)\n",
    "y = np.random.randn(100,50)\n",
    "sum_y = np.sum(y, axis=0, keepdims=True)\n",
    "y = y / sum_y # Create softmax-like distribution\n",
    "t = create_random_onehot(100,50)\n",
    "\n",
    "# Test forward\n",
    "myMSE = MeanSquaredError(t,y)\n",
    "myCE = CrossEntropyError(t,y)\n",
    "\n",
    "print(\"My Mean of Squared Error: \" + str(myMSE))\n",
    "print(\"My Cross Entropy Error: \" + str(myCE))\n",
    "\n",
    "yb = np.random.rand(100,50)\n",
    "tb = np.random.randint(2, size=(100,50))\n",
    "\n",
    "ybs = np.concatenate((yb, 1-yb))\n",
    "tbs = np.concatenate((tb, 1-tb))\n",
    "\n",
    "\n",
    "myBCE = BinaryCrossEntropyError(tb,yb)\n",
    "print(\"My Binary Cross Entropy Error: \" + str(myBCE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My Mean of Squared Error: 0.42921158147 <br/>\n",
    "My Cross Entropy Error: 15.8303339039 <br/>\n",
    "My Binary Cross Entropy Error: 1.04915015195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Mean of Squared Derivation dMSE_dy at dMSE_dy[2,4]=0.000000, dMSE_dy[6,5]=0.000618, dMSE_dy[8,20]=-0.000168, dMSE_dy[22,7]=0.000003, dMSE_dy[98,40]=0.000039\n",
      "My Cross Entropy Derivation: dCE_dy at dCE_dy[2,4]=0.000018, dCE_dy[6,5]=0.030883, dCE_dy[8,20]=-0.008391, dCE_dy[22,7]=0.000130, dCE_dy[98,40]=0.001969\n",
      "My Binar Cross Entropy Derivation: dBCE_dy at dBCE_dy[2]=0.100696, dBCE_dy[6]=-0.671816, dBCE_dy[20]=0.031237, dBCE_dy[22]=-0.106447, dBCE_dy[40]=-0.052379\n"
     ]
    }
   ],
   "source": [
    "# Test backward\n",
    "\n",
    "dMSE_dy1 = dMSE_dy(t, y)\n",
    "dCE_dy1 =  dCE_dy(t, y)\n",
    "dBCE_dy1 =  dBCE_dy(tb, yb)\n",
    "    \n",
    "print(\"My Mean of Squared Derivation dMSE_dy at dMSE_dy[2,4]=%f, dMSE_dy[6,5]=%f, dMSE_dy[8,20]=%f, dMSE_dy[22,7]=%f, dMSE_dy[98,40]=%f\" \\\n",
    "      %(dMSE_dy1[2,4], dMSE_dy1[6,5], dMSE_dy1[8,20], dMSE_dy1[22,7], dMSE_dy1[98,40]))\n",
    "print(\"My Cross Entropy Derivation: dCE_dy at dCE_dy[2,4]=%f, dCE_dy[6,5]=%f, dCE_dy[8,20]=%f, dCE_dy[22,7]=%f, dCE_dy[98,40]=%f\" \\\n",
    "      %(dCE_dy1[2,4], dCE_dy1[6,5], dCE_dy1[8,20], dCE_dy1[22,7], dCE_dy1[98,40]))\n",
    "print(\"My Binar Cross Entropy Derivation: dBCE_dy at dBCE_dy[2]=%f, dBCE_dy[6]=%f, dBCE_dy[20]=%f, dBCE_dy[22]=%f, dBCE_dy[40]=%f\" \\\n",
    "      %(dBCE_dy1[2], dBCE_dy1[6], dBCE_dy1[20], dBCE_dy1[22], dBCE_dy1[40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My Mean of Squared Derivation dMSE_dy at dMSE_dy[2,4]=0.000000, dMSE_dy[6,5]=0.000618, dMSE_dy[8,20]=-0.000168, dMSE_dy[22,7]=0.000003, dMSE_dy[98,40]=0.000039<br/>\n",
    "My Cross Entropy Derivation: dCE_dy at dCE_dy[2,4]=0.000018, dCE_dy[6,5]=0.030883, dCE_dy[8,20]=-0.008391, dCE_dy[22,7]=0.000130, dCE_dy[98,40]=0.001969<br/>\n",
    "My Binar Cross Entropy Derivation: dBCE_dy at dBCE_dy[2]=0.100696, dBCE_dy[6]=-0.671816, dBCE_dy[20]=0.031237, dBCE_dy[22]=-0.106447, dBCE_dy[40]=-0.052379<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Layer Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Initialization\n",
    "\n",
    "Initialize a layer: Use Xavier Initialization if the activation is sigmoid or tanh, otherwise use He Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(n_in, n_out, activation_string):\n",
    "    \"\"\"\n",
    "    Implement the initialization for our layer\n",
    "\n",
    "    Arguments:\n",
    "    n_in -- size of previous layer\n",
    "    n_out -- size of current layer\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_string == 'sigmoid' or activation_string == 'tanh' or activation_string == 'softmax':\n",
    "        # Xavier Initialization\n",
    "        W = np.random.randn(n_out, n_in) * (np.sqrt(2. / (n_in + n_out)))\n",
    "        b = np.zeros((n_out, 1))\n",
    "    else: \n",
    "        # He Initialization\n",
    "        W = np.random.randn(n_out, n_in) * (np.sqrt(1. / n_in))\n",
    "        b = np.zeros((n_out, 1))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A_prev, W, b, activation_string):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for our layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data X): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- values of (A_prev, Z, W) we store for computing backward propagation efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if activation_string == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif activation_string == 'tanh':\n",
    "        A = tanh(Z)\n",
    "    elif activation_string == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif activation_string == 'softmax':\n",
    "        A = softmax_batch(Z, axis=1)\n",
    "    else:\n",
    "        A = linear(Z)\n",
    "    \n",
    "    cache = (A_prev, Z, W, activation_string)\n",
    "    \n",
    "    return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test forward__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: Z = [[ 1.1337088  -1.20334105]] , A = [[ 0.75652269  0.2308814 ]]\n",
      "With tanh: A = [[ 0.81228478 -0.83467086]]\n",
      "With ReLU: A = [[ 1.1337088  0.       ]]\n",
      "With softmax: A = [[ 0.91189936  0.08810064]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2222)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "\n",
    "A, cache = forward(A_prev, W, b, \"sigmoid\")\n",
    "print(\"With sigmoid: Z = %s , A = %s\" % (str(cache[1]), str(A)))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"tanh\")\n",
    "print(\"With tanh: A = \" + str(A))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"softmax\")\n",
    "print(\"With softmax: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "With sigmoid: Z = [[ 1.1337088  -1.20334105]] , A = [[ 0.75652269  0.2308814 ]]<br/>\n",
    "With tanh: A = [[ 0.81228478 -0.83467086]]<br/>\n",
    "With ReLU: A = [[ 1.1337088  0.       ]]<br/>\n",
    "With softmax: A = [[ 0.91189936  0.08810064]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for our layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- activation gradient for current layer l \n",
    "    cache -- values of (A_prev, Z, W) we store for computing backward propagation efficiently\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, Z, W, activation_string = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation_string == 'sigmoid':\n",
    "        dZ = dA * sigmoid_prime(Z)\n",
    "    elif activation_string == 'tanh':\n",
    "        dZ = dA * tanh_prime(Z)\n",
    "    elif activation_string == 'relu':\n",
    "        dZ = dA * relu_prime(Z)\n",
    "    elif activation_string == 'softmax':  # This is hard, I did it for you\n",
    "        dZ = dA * softmax_batch(Z, axis=1)\n",
    "        s = dZ.sum(axis=dZ.ndim - 1, keepdims=True)\n",
    "        dZ -= s * softmax_batch(Z, axis=1)\n",
    "    else:\n",
    "        dZ = dA * linear_prime(Z)\n",
    "    \n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Backward__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.38601836  0.4093035 ]\n",
      " [ 0.04975095  0.05275199]\n",
      " [ 0.37804364  0.40084773]]\n",
      "dW = [[ 0.01722623 -0.1803237   0.09144594]]\n",
      "db = [[-0.25163402]]\n",
      "\n",
      "tanh:\n",
      "dA_prev = [[ 0.74460672  1.47719476]\n",
      " [ 0.09596665  0.19038431]\n",
      " [ 0.72922394  1.4466775 ]]\n",
      "dW = [[ 0.22431658 -0.34262738  0.38665635]]\n",
      "db = [[-0.70296173]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 2.05442539  0.        ]\n",
      " [ 0.26477914  0.        ]\n",
      " [ 2.01198317  0.        ]]\n",
      "dW = [[-0.51363355 -0.97619193 -0.17936819]]\n",
      "db = [[-0.65000516]]\n",
      "\n",
      "softmax:\n",
      "dA_prev = [[ 0.05452866 -0.05452866]\n",
      " [ 0.00702778 -0.00702778]\n",
      " [ 0.05340215 -0.05340215]]\n",
      "dW = [[-0.02878513 -0.02632298 -0.02143347]]\n",
      "db = [[ -1.38777878e-17]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1111)\n",
    "dA = np.random.randn(1,2)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z = np.random.randn(1,2)\n",
    "\n",
    "cache_sigmoid = (A_prev, Z, W, \"sigmoid\")\n",
    "cache_tanh = (A_prev, Z, W, \"tanh\")\n",
    "cache_relu = (A_prev, Z, W, \"relu\")\n",
    "cache_softmax = (A_prev, Z, W, \"softmax\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_sigmoid)\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_tanh)\n",
    "print (\"tanh:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_relu)\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_softmax)\n",
    "print (\"softmax:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "sigmoid:<br/>\n",
    "dA_prev = [[ 0.38601836  0.4093035 ]<br/>\n",
    " [ 0.04975095  0.05275199]<br/>\n",
    " [ 0.37804364  0.40084773]]<br/>\n",
    "dW = [[-0.01627613 -0.14937127  0.091592  ]]<br/>\n",
    "db = [[-0.25163402]]<br/>\n",
    "<br/>\n",
    "tanh:<br/>\n",
    "dA_prev = [[ 0.74460672  1.47719476]<br/>\n",
    " [ 0.09596665  0.19038431]<br/>\n",
    " [ 0.72922394  1.4466775 ]]<br/>\n",
    "dW = [[-0.25178866 -0.44375637  0.44336361]]<br/>\n",
    "db = [[-0.70296173]]<br/>\n",
    "<br/>\n",
    "relu:<br/>\n",
    "dA_prev = [[ 2.05442539  0.        ]<br/>\n",
    " [ 0.26477914  0.        ]<br/>\n",
    " [ 2.01198317  0.        ]]<br/>\n",
    "dW = [[ 0.6115193  -0.30198246 -0.35733097]]<br/>\n",
    "db = [[-0.65000516]]<br/>\n",
    "\n",
    "softmax:<br/>\n",
    "dA_prev = [[ 0.05452866 -0.05452866]<br/>\n",
    " [ 0.00702778 -0.00702778]<br/>\n",
    " [ 0.05340215 -0.05340215]]<br/>\n",
    "dW = [[-0.02878513 -0.02632298 -0.02143347]]<br/>\n",
    "db = [[ -1.38777878e-17]]<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Update parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(lr, W, b, dW, db):\n",
    "    W1 = W - lr * dW \n",
    "    b1 = b - lr * db\n",
    "    return W1, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Network Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_initialize(layer_sizes = [768,100,50,10], activations = [\"relu\",\"relu\",\"softmax\"], seed=9999):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of a network.\n",
    "    \n",
    "    Arguments:\n",
    "    layer_sizes -- A list of layer sizes. \n",
    "    activations -- A list of corresponding activation functions.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Dictionary of initialized weights and biases for every layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    parameters = {} \n",
    "    \n",
    "    for l in range(1, len(layer_sizes)):\n",
    "        parameters['W' + str(l)], parameters['b' + str(l)] = initialize(layer_sizes[l-1], layer_sizes[l], activations[l-1])\n",
    "        parameters['act' + str(l)] = activations[l-1]\n",
    "        \n",
    "    return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'W1': array([[-0.36418784,  0.41853418],\n",
      "       [ 0.03385161, -0.34535427],\n",
      "       [-0.25098956, -0.27705387]]),\n",
      "    'W2': array([[ 0.99648945, -0.73309211,  1.18999424],\n",
      "       [-0.06313226,  0.06406165,  0.09760321],\n",
      "       [-0.35157642, -0.87994782, -0.61020235],\n",
      "       [-0.76922564,  0.39821514, -0.92965227]]),\n",
      "    'act1': 'relu',\n",
      "    'act2': 'softmax',\n",
      "    'b1': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]),\n",
      "    'b2': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "parameters = network_initialize([2,3,4],[\"relu\",\"softmax\"])\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "{ &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W1': array([[-0.36418784, &nbsp;  &nbsp;  0.41853418], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [ 0.03385161, &nbsp;  &nbsp; -0.34535427], <br/>\n",
    "   &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.25098956, &nbsp;  &nbsp; -0.27705387]]), <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W2': array([[ 0.99648945, &nbsp;  &nbsp; -0.73309211, &nbsp;  &nbsp;  1.18999424], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [-0.06313226,  &nbsp;  &nbsp; 0.06406165, &nbsp;  &nbsp;  0.09760321], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.35157642, &nbsp;  &nbsp; -0.87994782, &nbsp;  &nbsp; -0.61020235], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.76922564, &nbsp;  &nbsp;  0.39821514, &nbsp;  &nbsp; -0.92965227]]), <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;    'act1': 'relu', <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;     'act2': 'softmax', <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;     'b1': array([[ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.]]), <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;   'b2': array([[ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.]])} <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Network Forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Do the forward pass through the layers.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- The input of the size (n, m): n features and m instances (m = batch size).\n",
    "    parameters -- The weights and biases of every layers in the network.\n",
    "    \n",
    "    Returns:\n",
    "    A -- Final activations (output activations).\n",
    "    caches -- the cached values for faster calculation of the backward step later.\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev = X\n",
    "    L = len(parameters) // 3 # Number of layers in the network\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(L):\n",
    "        W = parameters['W' + str(l+1)]\n",
    "        b = parameters['b' + str(l+1)]\n",
    "        activation_string = parameters['act' + str(l+1)]\n",
    "        A, cache = forward(A_prev, W, b, activation_string)\n",
    "        caches.append(cache)\n",
    "        A_prev = A\n",
    "    \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[ 0.01497328  0.87662976  0.09019153  0.01820543]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(5,4)\n",
    "W1 = np.random.randn(4,5)\n",
    "b1 = np.random.randn(4,1)\n",
    "W2 = np.random.randn(3,4)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(1,3)\n",
    "b3 = np.random.randn(1,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"relu\",\n",
    "              \"act3\": \"softmax\"}\n",
    "AL, caches = network_forward(X, parameters)    \n",
    "print(\"AL = \" + str(AL))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "AL = [[ 0.01497328  0.87662976  0.09019153  0.01820543]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Calculate the error and error term of the last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(T, AL, error_string):\n",
    "    \"\"\"\n",
    "    Calculate the error and the error term of the last layer.\n",
    "    \n",
    "    Arguments:\n",
    "    T -- The target labels of the size (n, m): n features and m instances (m = batch size).\n",
    "    AL -- Final activations (output activations from the last layer).\n",
    "    error_string -- The string representing the error function: \"bce\", \"mse\" or \"ce\".\n",
    "    \n",
    "    Returns:\n",
    "    error -- The error value.\n",
    "    dAL -- the error term (the derivative of the error w.r.t the activation) of the last layer.\n",
    "    \"\"\"\n",
    "    if error_string == \"mse\":\n",
    "        error = MeanSquaredError(T, AL)\n",
    "        dAL = dMSE_dy(T, AL)\n",
    "    elif error_string == 'bce':\n",
    "        error = BinaryCrossEntropyError(T, AL)\n",
    "        dAL = dBCE_dy(T, AL)\n",
    "    elif error_string == 'ce':\n",
    "        error = CrossEntropyError(T, AL)\n",
    "        dAL = dCE_dy(T, AL)\n",
    "    else:\n",
    "        raise NameError(\"Your error string '%s' is undefined!\" % error_string)\n",
    "        \n",
    "    return error, dAL\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Network Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_backward(dAL, caches):\n",
    "    \"\"\"\n",
    "    Do the backward pass through the layers.\n",
    "    \n",
    "    Arguments:\n",
    "    dAL -- the error term (the derivative of the error w.r.t the activation) of the last layer.\n",
    "    caches -- the cached values from the forward step before.\n",
    "   \n",
    "    Returns:\n",
    "    grads -- a dictionary to store the calculated derivatives of the error w.r.t to the weights and biases.\n",
    "    \"\"\"\n",
    "    L = len(caches) # Number of layers in the network\n",
    "    grads = {}\n",
    "    \n",
    "    dA = dAL\n",
    "    \n",
    "    for l in reversed(range(1,L+1)):\n",
    "        cache_l = caches[l-1]\n",
    "        dA, grads[\"dW\" + str(l)], grads[\"db\" + str(l)] = backward(dA, cache_l)\n",
    "        grads[\"dA\" + str(l-1)] = dA\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My activation last layer AL:\n",
      " [[ 0.21921768  0.11988903  0.16260051  0.12001303  0.24447803  0.13380173]]\n",
      "========================\n",
      " My error:1.83258646479\n",
      "========================\n",
      "My W1 grad dW1:\n",
      "tensor([[-0.0010, -0.0042,  0.0002,  0.0012, -0.0027, -0.0008,  0.0011],\n",
      "        [-0.0017,  0.0006,  0.0001,  0.0011,  0.0006,  0.0009,  0.0007],\n",
      "        [-0.0009,  0.0015, -0.0012, -0.0013,  0.0005,  0.0006,  0.0004],\n",
      "        [ 0.0012, -0.0007,  0.0009,  0.0004, -0.0005, -0.0013, -0.0013],\n",
      "        [ 0.0002, -0.0002,  0.0003,  0.0005,  0.0001, -0.0000, -0.0003]],\n",
      "       dtype=torch.float64)\n",
      "My b1 grad db1:\n",
      "tensor([[-0.0037],\n",
      "        [ 0.0013],\n",
      "        [-0.0020],\n",
      "        [ 0.0017],\n",
      "        [ 0.0007]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(7,6)\n",
    "W1 = np.random.randn(5,7)\n",
    "b1 = np.random.randn(5,1)\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(1,3)\n",
    "b3 = np.random.randn(1,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"sigmoid\",\n",
    "              \"act3\": \"softmax\"}\n",
    "\n",
    "t = create_random_onehot(1,6)\n",
    "\n",
    "\n",
    "\n",
    "AL, caches = network_forward(X, parameters)\n",
    "\n",
    "print(\"My activation last layer AL:\\n\", AL)\n",
    "error, dAL = calculate_error(t, AL, \"ce\")\n",
    "\n",
    "\n",
    "print(\"========================\")\n",
    "print(\" My error:\" + str(error))\n",
    "\n",
    "my_A2, my_Z3, _, _ = caches[2]\n",
    "\n",
    "#my_dZ3 = dAL * sigmoid_prime(my_Z3)\n",
    "\n",
    "my_dZ3 = dAL * softmax(my_Z3)\n",
    "s = my_dZ3.sum(axis=my_dZ3.ndim - 1, keepdims=True)\n",
    "my_dZ3 -= s * softmax(my_Z3) \n",
    "\n",
    "\n",
    "\n",
    "m = 1 # my_A2.shape[1]\n",
    "my_dW3 = np.dot(my_dZ3, my_A2.T)\n",
    "my_db3 = np.sum(my_dZ3, axis=1, keepdims=True)\n",
    "my_dA2 = np.dot(W3.T, my_dZ3)\n",
    "\n",
    "\n",
    "my_A1, my_Z2, _, _ = caches[1]\n",
    "my_dZ2 = my_dA2 * relu_prime(my_Z2)\n",
    "my_dW2 = np.dot(my_dZ2, my_A1.T)\n",
    "my_db2 = np.sum(my_dZ2, axis=1, keepdims=True)\n",
    "my_dA1 = np.dot(W2.T, my_dZ2)\n",
    "\n",
    "my_A0, my_Z1, _, _ = caches[0]\n",
    "my_dZ1 = my_dA1 * relu_prime(my_Z1)\n",
    "my_dW1 = np.dot(my_dZ1, my_A0.T)\n",
    "my_db1 = np.sum(my_dZ1, axis=1, keepdims=True)\n",
    "my_dA0 = np.dot(W1.T, my_dZ1)\n",
    "\n",
    "\n",
    "print(\"========================\")\n",
    "print(\"My W1 grad dW1:\\n\" + str(torch.from_numpy(my_dW1))) \n",
    "print(\"My b1 grad db1:\\n\" + str(torch.from_numpy(my_db1)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My activation last layer AL: <br/>\n",
    " &nbsp;  &nbsp; [[ 0.21921768  0.11988903  0.16260051  0.12001303  0.24447803  0.13380173]] <br/>\n",
    "======================== <br/>\n",
    " &nbsp; My error:1.83258646479 <br/>\n",
    "======================== <br/>\n",
    "My W1 grad dW1: <br/>\n",
    "tensor([[-0.0010, -0.0042,  0.0002,  0.0012, -0.0027, -0.0008,  0.0011], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [-0.0017,  0.0006,  0.0001,  0.0011,  0.0006,  0.0009,  0.0007], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;        [-0.0009,  0.0015, -0.0012, -0.0013,  0.0005,  0.0006,  0.0004], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;          [ 0.0012, -0.0007,  0.0009,  0.0004, -0.0005, -0.0013, -0.0013], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [ 0.0002, -0.0002,  0.0003,  0.0005,  0.0001, -0.0000, -0.0003]], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp;        dtype=torch.float64) <br/>\n",
    "My b1 grad db1: <br/>\n",
    "tensor([[-0.0037], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [ 0.0013], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [-0.0020], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [ 0.0017], <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;         [ 0.0007]], dtype=torch.float64) <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4. Update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_update(lr, parameters, grads):\n",
    "    \"\"\"\n",
    "    Update the parameters of all layers.\n",
    "    \n",
    "    Arguments:\n",
    "    lr -- learning rate.\n",
    "    parameters -- the parameters of the network to be updated.\n",
    "    grads -- a dictionary to store the calculated derivatives of the error w.r.t to the parameters.\n",
    "   \n",
    "    Returns:\n",
    "    parameters -- the updated parameters of the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 3 # Number of layers in the network\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= lr * grads[\"dW\" + str(l+1)] \n",
    "        parameters[\"b\" + str(l+1)] -= lr * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(7,10)\n",
    "W1 = np.random.randn(5,7)\n",
    "b1 = np.random.randn(5,1)\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.random.randn(3,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"softmax\"}\n",
    "\n",
    "t = create_random_onehot(3,10)\n",
    "\n",
    "\n",
    "AL, caches = network_forward(X, parameters)\n",
    "print(np.sum(AL,axis=0))\n",
    "error, dAL = calculate_error(t, AL, \"ce\")\n",
    "grads = network_backward(dAL, caches)\n",
    "\n",
    "parameters = network_update(0.5, parameters, grads)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(parameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "{&nbsp;  &nbsp;  &nbsp;    'W1': array([[-1.03295651,  0.35862528,  1.07353604, -0.37522946,  0.39616932, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;       -0.47148865,  2.33632501], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 1.50286388, -0.59545956,  0.52839833,  0.93994217,  0.42634415, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;        -0.75807972, -0.16243501], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [ 0.72680932,  0.44408241, -0.85682286,  0.44692782, -1.01464803, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;      -2.13232319,  0.17386349], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.95139218,  0.4419052 ,  1.46915517,  1.7497915 ,  0.35367386, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;      -0.64316151, -0.04739572], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [-1.44902964, -0.03619117, -0.09084896,  0.17629557,  1.09461738, <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;      -2.12647575,  0.75144374]]), <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W2': array([[-0.73957208,  0.52294535, -0.59187648, -0.47748711,  0.11252968], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 1.90396151,  0.69458768, -0.01951732,  1.66320563,  0.02993579], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [-0.29749921, -0.96813764,  0.16706695,  0.11660199, -0.68225729]]), <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;    'act1': 'relu', <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;   'act2': 'softmax', <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;    'b1': array([[-0.54084024], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.79330547], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.17365274], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [-1.03523086], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.87426565]]), <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;    'b2': array([[-1.91402115], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [-0.13990231], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;     [ 0.11492465]])} <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Read MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 785)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../Data/mnist_seven.csv\"\n",
    "data = np.genfromtxt(data_path, delimiter=\",\", dtype=\"uint8\")\n",
    "train, dev, test = data[:4000], data[4000:4500], data[4500:]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    X = dataset[:, 1:] / 255.     # Normalize input features\n",
    "    Y_temp = dataset[:, 0]\n",
    "    \n",
    "    # Convert labels to one-hot vectors\n",
    "    n_values = np.max(Y_temp) + 1\n",
    "    Y = np.eye(n_values)[Y_temp]\n",
    "\n",
    "    return X.T, Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 4000)\n",
      "(10, 4000)\n",
      "(784, 500)\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = normalize(train)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_test, Y_test = normalize(test)\n",
    "print(X_test.shape)\n",
    "print(Y_test[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Error of the epoch 1: 2.962905431085156\n",
      "Error of the epoch 2: 2.8822520623100774\n",
      "Error of the epoch 3: 2.707589912648429\n",
      "Error of the epoch 4: 2.383859018345201\n",
      "Error of the epoch 5: 2.0588371308092124\n",
      "Error of the epoch 6: 1.8203479814149524\n",
      "Error of the epoch 7: 1.6940501042316065\n",
      "Error of the epoch 8: 1.6214515998747185\n",
      "Error of the epoch 9: 1.5838952386206195\n",
      "Error of the epoch 10: 1.564293865817791\n",
      "Error of the epoch 11: 1.551468370175875\n",
      "Error of the epoch 12: 1.5403775436200073\n",
      "Error of the epoch 13: 1.530073374608231\n",
      "Error of the epoch 14: 1.520545323572489\n",
      "Error of the epoch 15: 1.5115586997830732\n",
      "Error of the epoch 16: 1.5028758793058878\n",
      "Error of the epoch 17: 1.4944096332868608\n",
      "Error of the epoch 18: 1.486165634186042\n",
      "Error of the epoch 19: 1.4781704261768858\n",
      "Error of the epoch 20: 1.4704354874723946\n",
      "Error of the epoch 21: 1.4629529234322995\n",
      "Error of the epoch 22: 1.4557048780587873\n",
      "Error of the epoch 23: 1.4486733017720832\n",
      "Error of the epoch 24: 1.4418458137971817\n",
      "Error of the epoch 25: 1.435217499341101\n",
      "Error of the epoch 26: 1.4287897581342401\n",
      "Error of the epoch 27: 1.4225676578938486\n",
      "Error of the epoch 28: 1.4165568678740246\n",
      "Error of the epoch 29: 1.410760842384177\n",
      "Error of the epoch 30: 1.405178877106315\n",
      "Error of the epoch 31: 1.3998056237425274\n",
      "Error of the epoch 32: 1.3946321328474292\n",
      "Error of the epoch 33: 1.3896476575217076\n",
      "Error of the epoch 34: 1.3848410880318653\n",
      "Error of the epoch 35: 1.3802014423283224\n",
      "Error of the epoch 36: 1.3757177366042552\n",
      "Error of the epoch 37: 1.3713789067346858\n",
      "Error of the epoch 38: 1.367174087122657\n",
      "Error of the epoch 39: 1.3630930616657075\n",
      "Error of the epoch 40: 1.3591265759443079\n",
      "Error of the epoch 41: 1.3552663964036622\n",
      "Error of the epoch 42: 1.3515052230889126\n",
      "Error of the epoch 43: 1.3478366234781312\n",
      "Error of the epoch 44: 1.344255062040112\n",
      "Error of the epoch 45: 1.3407559715041435\n",
      "Error of the epoch 46: 1.3373357636668513\n",
      "Error of the epoch 47: 1.3339917288226966\n",
      "Error of the epoch 48: 1.3307218538111756\n",
      "Error of the epoch 49: 1.327524627195098\n",
      "Error of the epoch 50: 1.3243988770011885\n",
      "Error of the epoch 51: 1.3213436275188786\n",
      "Error of the epoch 52: 1.3183579118824016\n",
      "Error of the epoch 53: 1.3154404862665672\n",
      "Error of the epoch 54: 1.3125894936767972\n",
      "Error of the epoch 55: 1.3098022825240878\n",
      "Error of the epoch 56: 1.307075640696184\n",
      "Error of the epoch 57: 1.3044065101328124\n",
      "Error of the epoch 58: 1.3017929174755627\n",
      "Error of the epoch 59: 1.299234719461924\n",
      "Error of the epoch 60: 1.2967339521194257\n",
      "Error of the epoch 61: 1.2942948978738187\n",
      "Error of the epoch 62: 1.2919239552905057\n",
      "Error of the epoch 63: 1.2896287372060835\n",
      "Error of the epoch 64: 1.2874157170361713\n",
      "Error of the epoch 65: 1.2852871655953977\n",
      "Error of the epoch 66: 1.2832394154844247\n",
      "Error of the epoch 67: 1.28126366880409\n",
      "Error of the epoch 68: 1.2793485955569661\n",
      "Error of the epoch 69: 1.277483206901662\n",
      "Error of the epoch 70: 1.2756588996431786\n",
      "Error of the epoch 71: 1.2738703735773818\n",
      "Error of the epoch 72: 1.2721156980877537\n",
      "Error of the epoch 73: 1.270395639561471\n",
      "Error of the epoch 74: 1.268711995291737\n",
      "Error of the epoch 75: 1.2670651764512497\n",
      "Error of the epoch 76: 1.265452581599495\n",
      "Error of the epoch 77: 1.2638692244372862\n",
      "Error of the epoch 78: 1.2623097721412462\n",
      "Error of the epoch 79: 1.2607700141887137\n",
      "Error of the epoch 80: 1.2592471969526735\n",
      "Error of the epoch 81: 1.2577398067463144\n",
      "Error of the epoch 82: 1.2562472614173503\n",
      "Error of the epoch 83: 1.2547696081688466\n",
      "Error of the epoch 84: 1.2533071992033984\n",
      "Error of the epoch 85: 1.2518603395901666\n",
      "Error of the epoch 86: 1.2504289601825258\n",
      "Error of the epoch 87: 1.2490124044653255\n",
      "Error of the epoch 88: 1.2476093988057095\n",
      "Error of the epoch 89: 1.2462182041050036\n",
      "Error of the epoch 90: 1.2448368713340718\n",
      "Error of the epoch 91: 1.2434634968670184\n",
      "Error of the epoch 92: 1.2420964041461948\n",
      "Error of the epoch 93: 1.240734231257911\n",
      "Error of the epoch 94: 1.2393759436227427\n",
      "Error of the epoch 95: 1.238020806539194\n",
      "Error of the epoch 96: 1.2366683520002608\n",
      "Error of the epoch 97: 1.2353183690847753\n",
      "Error of the epoch 98: 1.2339709428203824\n",
      "Error of the epoch 99: 1.2326265614426295\n",
      "Error of the epoch 100: 1.2312862985374093\n",
      "...Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "# Train\n",
    "print(\"Training...\")\n",
    "np.random.seed(1234)\n",
    "parameters = network_initialize(layer_sizes = [784,100,50,10], activations = [\"sigmoid\", \"sigmoid\", \"softmax\"])\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    L = 0\n",
    "    \n",
    "    # Split into minibatches into a *list* of sub-arrays\n",
    "    # we want to split along the number of instances, so axis = 1\n",
    "    X_minibatch = np.array_split(X_train, batch_size, axis = 1)\n",
    "    Y_minibatch = np.array_split(Y_train, batch_size, axis = 1) \n",
    "\n",
    "    # We shuffle the minibatches of X and Y in the same way\n",
    "    nmb = len(X_minibatch) # number of minibatches\n",
    "    np.random.seed(8888)\n",
    "    shuffled_index = np.random.permutation(range(nmb))\n",
    "\n",
    "    # Now we can do the training, we cannot vectorize over different minibatches\n",
    "    # They are like our \"epochs\"\n",
    "    for i in range(nmb):\n",
    "        X_current = X_minibatch[shuffled_index[i]]\n",
    "        Y_current = Y_minibatch[shuffled_index[i]]         \n",
    "            \n",
    "    #   Those two commented lines are for training Batch GD   \n",
    "    #   AL, caches = network_forward(X_train, parameters)\n",
    "    #   error, dAL = calculate_error(Y_train, AL, \"ce\")\n",
    "        AL, caches = network_forward(X_current, parameters)\n",
    "        error, dAL = calculate_error(Y_current, AL, \"ce\")\n",
    "        grads = network_backward(dAL, caches)\n",
    "        parameters = network_update(15, parameters, grads)\n",
    "        L += error\n",
    "        \n",
    "    print(\"Error of the epoch {0}: {1}\".format(epoch + 1, L/batch_size))\n",
    "\n",
    "print(\"...Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test images: 93 %\n"
     ]
    }
   ],
   "source": [
    "m_test = X_test.shape[1]\n",
    "AL_test, _ = network_forward(X_test, parameters)\n",
    "correct = (np.argmax(Y_test, axis=0) == np.argmax(AL_test, axis=0)).sum()\n",
    "   \n",
    "print('Accuracy on the test images: %d %%' % (100 * correct / m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should produce a 93% accuracy, which is not bad. Just a little bit worse than the pytorch version (94%) whose difference is the usage of Adam (a better optimization technique) compared to our simple SGD. Notice the learning rate. Change it and play with it. In our simple SGD-based model, it is an important hyperparameter and the performance of our model is learning rate-sensitive. **Do you agree with me that PyTorch already helps us a lot on this simple feed-forward architecture?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
